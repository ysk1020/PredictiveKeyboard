# -*- coding: utf-8 -*-
"""Predictive Keyboard Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ryzAbCx3j8u5J-GusC7d46WnmU4IO3e8

# Predictive Keyboard Model

- The Tutorial according the project is going on-[here](https://amanxai.com/2025/05/13/building-a-predictive-keyboard-model-with-pytorch/)
- ChatGPT for detialed explination [here](https://)

## Importing libararies
"""

import nltk
from nltk.corpus import wordnet as wn
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

import torch
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('punkt_tab')

import numpy as np
import pandas as pd

"""## Loading dataset"""

with open('/content/drive/MyDrive/Colab Notebooks/sherlock-holm.es_stories_plain-text_advs.txt') as f:
  text=f.read().lower()

tokens=word_tokenize(text)
print('total tokens: ', len(tokens))

"""## Creating Vocabulary

1.   a dictionary to map each word to an index
2.   and another dictionary to reverse it back

Here, we counted how often each word appears using Counter, then sorted the vocabulary from most to least frequent. This sorted list helps us assign lower indices to more common words (useful for embeddings). Then, we created word2idx and idx2word dictionaries to convert words to unique IDs and back. Finally, we stored the total vocabulary size, which will define the input and output dimensions for our model.
"""

from collections import Counter

word_counts =  Counter(tokens)
vocab = sorted(word_counts, key=word_counts.get, reverse=True)

word2idx = {word: i for i, word in enumerate(vocab)}
idx2word = {i: word for i, word in enumerate(vocab)}
vocab_size = len(vocab)

print(vocab_size)
print(vocab)
print(word2idx)
print(idx2word)

"""## Input-output sequences

To predict the next word, the model needs context. We can use a sliding window approach. So, let’s create input-target sequences for next word prediction:
"""

sequence_length = 4

data = []
for i in range(len(tokens)-sequence_length):
  input_seq = tokens[i:i+sequence_length]
  target_word = tokens[i+sequence_length]
  data.append((input_seq, target_word))

print(data[:5])

def encode(seq): return [word2idx[word] for word in seq]

encoded_data = [(torch.tensor(encode(inp)), torch.tensor(word2idx[target])) for inp, target in data]

"""1. A tensor is like a massive container of numbers (often multi-dimensional) that holds your data in a structured form;

2. The shape tells the model how that data is organized — for example:

    * how many sequences per batch,
    * how long each sequence is,
how many features (like embedding dimensions) each element has.

  yes — the shape defines what kind of data structure the layer expects and outputs.

## Designing the model architecture
"""

import torch.nn as nn

class PredictiveKeyboard(nn.Module):
  def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):
    super(PredictiveKeyboard, self).__init__()
    self.embedding = nn.Embedding(vocab_size,embed_dim)
    self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
    self.fc=nn.Linear(hidden_dim, vocab_size)

  def forward(self,x):
    x = self.embedding(x)              # converts word indices into dense vectors.
    output, _ = self.lstm(x)           # captures the sequential context of the input.
    output = self.fc(output[:, -1, :]) # feed to get a vector of size "vocab_size", respresenting the predicted probabilities for each word in the vocabulary
    return output

"""## Training the model"""

import torch
import torch.optim as optim
import random

model = PredictiveKeyboard(vocab_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001 )

epochs = 20
for epoch in range(epochs):
    total_loss = 0
    random.shuffle(encoded_data)
    for input_seq, target in encoded_data[:10000]:
        input_seq = input_seq.unsqueeze(0)
        output = model(input_seq)
        loss = criterion(output, target.unsqueeze(0))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(encoded_data)}')

"""## Predicting the next words"""

import torch.nn.functional as F

def suggest_next_words(model, text_prompt, top_k=3):
  model.eval()
  tokens = word_tokenize(text_prompt.lower())
  if len(tokens) < sequence_length:
    raise ValueError(f"Text prompt should have at least {sequence_length-1} words.")

  input_seq = tokens[-(sequence_length-1):]
  input_tensor = torch.tensor(encode(input_seq)).unsqueeze(0)

  with torch.no_grad():
    output = model(input_tensor)
    probs = F.softmax(output, dim=1).squeeze()
    top_indices = torch.topk(probs, k=top_k).indices.tolist()

  return [idx2word[idx] for idx in top_indices]
print('Suggestions:', suggest_next_words(model, 'So, are we really at'))